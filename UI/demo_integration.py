"""
Demo Integration Script for BenefitsFlow
Shows how to integrate the Streamlit UI with your existing RAG pipeline.
"""

import sys
import os
from pathlib import Path

# Add the parent directory to the path to import your existing modules
sys.path.append(str(Path(__file__).parent.parent))

def demo_rag_integration():
    """
    Demo function showing how to integrate with your existing RAG pipeline.
    This replaces the mock responses in rag_backend.py with real API calls.
    """
    
    print("üîó BenefitsFlow RAG Integration Demo")
    print("=" * 50)
    
    # Example of how to integrate with your existing modules
    try:
        # Import your existing RAG modules (uncomment when ready)
        # from src.03_rag_query import query_rag_system
        # from src.02_index_docs import get_vector_store
        
        print("‚úÖ Successfully imported existing RAG modules")
        
        # Example integration code:
        integration_code = '''
# In rag_backend.py, replace the mock get_rag_response function with:

def get_rag_response(prompt: str, conversation_history: List[Dict], user_context: Dict):
    """
    Real RAG integration with your existing pipeline
    """
    try:
        # 1. Prepare query with conversation context
        context = " ".join([msg["content"] for msg in conversation_history[-5:]])  # Last 5 messages
        full_query = f"Context: {context}\\n\\nQuery: {prompt}"
        
        # 2. Query your vector database
        from src.03_rag_query import query_rag_system
        results = query_rag_system(full_query, top_k=5)
        
        # 3. Generate response using your LLM
        response_text = generate_llm_response(prompt, results, user_context)
        
        # 4. Extract sources and metadata
        sources = extract_sources_from_results(results)
        programs = extract_programs_from_response(response_text)
        
        return response_text, sources, programs
        
    except Exception as e:
        # Fallback to mock responses if RAG system is unavailable
        return get_mock_response(prompt)

def generate_llm_response(prompt: str, rag_results: List[Dict], user_context: Dict):
    """
    Generate response using your trained LLM
    """
    # Use your existing LLM API or model
    # from trainer.llm_api import generate_response
    
    context_docs = "\\n\\n".join([doc["content"] for doc in rag_results])
    
    system_prompt = f"""
    You are BenefitsFlow, a helpful assistant for California public benefits.
    Use the following context to answer the user's question:
    
    Context: {context_docs}
    
    Guidelines:
    - Be empathetic and supportive
    - Use plain language
    - Provide specific, actionable information
    - Always cite sources
    - If unsure, suggest contacting official sources
    """
    
    # Call your LLM API
    # response = generate_response(system_prompt, prompt)
    # return response
    
    # For now, return a placeholder
    return "This would be generated by your LLM with the RAG context."

def extract_sources_from_results(rag_results: List[Dict]):
    """
    Extract source information from RAG results
    """
    sources = []
    for result in rag_results:
        if "source" in result:
            sources.append({
                "name": result["source"].get("name", "Official Source"),
                "url": result["source"].get("url", "#"),
                "date": result["source"].get("date", "2025")
            })
    return sources

def extract_programs_from_response(response_text: str):
    """
    Extract mentioned programs from the response
    """
    programs = []
    program_keywords = {
        "CalFresh": ["calfresh", "food stamps", "food assistance"],
        "Medi-Cal": ["medi-cal", "health insurance", "health coverage"],
        "Unemployment Insurance": ["unemployment", "ui benefits", "job loss"],
        "CalWORKs": ["calworks", "cash assistance", "temporary aid"],
        "Section 8": ["section 8", "housing voucher", "rental assistance"]
    }
    
    response_lower = response_text.lower()
    for program, keywords in program_keywords.items():
        if any(keyword in response_lower for keyword in keywords):
            programs.append(program)
    
    return programs
        '''
        
        print("üìù Integration code example:")
        print(integration_code)
        
    except ImportError as e:
        print(f"‚ö†Ô∏è  Could not import existing modules: {e}")
        print("This is expected if you haven't set up the RAG pipeline yet.")
        print("The UI will work with mock responses until you integrate your RAG system.")

def demo_configuration():
    """
    Demo function showing how to configure the application
    """
    
    print("\n‚öôÔ∏è Configuration Demo")
    print("=" * 30)
    
    config_example = '''
# Environment Variables to Set:
export OPENAI_API_KEY="your-openai-api-key"
export CHROMA_PERSIST_DIRECTORY="./chroma_db"
export STREAMLIT_SERVER_PORT="8501"

# Or create a .env file:
OPENAI_API_KEY=your-openai-api-key
CHROMA_PERSIST_DIRECTORY=./chroma_db
STREAMLIT_SERVER_PORT=8501
    '''
    
    print("üîß Configuration setup:")
    print(config_example)

def demo_deployment():
    """
    Demo function showing deployment options
    """
    
    print("\nüöÄ Deployment Options")
    print("=" * 25)
    
    deployment_options = '''
1. Streamlit Cloud (Recommended for demos):
   - Push to GitHub
   - Connect to Streamlit Cloud
   - Automatic deployment

2. Heroku:
   - Add Procfile: web: streamlit run app.py --server.port=$PORT
   - Add requirements.txt
   - Deploy with git push

3. Docker:
   - Create Dockerfile
   - Build and run container
   - Deploy to any container platform

4. Local Development:
   - python run.py
   - Access at http://localhost:8501
    '''
    
    print(deployment_options)

def main():
    """
    Main demo function
    """
    print("üåç BenefitsFlow Integration Demo")
    print("=" * 40)
    
    demo_rag_integration()
    demo_configuration()
    demo_deployment()
    
    print("\n‚úÖ Demo complete!")
    print("\nNext steps:")
    print("1. Set up your RAG pipeline (src/ directory)")
    print("2. Update rag_backend.py with real API calls")
    print("3. Configure environment variables")
    print("4. Run: python run.py")
    print("5. Deploy to your preferred platform")

if __name__ == "__main__":
    main()
